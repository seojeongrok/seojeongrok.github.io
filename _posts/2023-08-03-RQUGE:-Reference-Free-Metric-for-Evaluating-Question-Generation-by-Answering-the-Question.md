---
title: RQUGE Reference-Free Metric for Evaluating Question Generation by Answering the Question
author: jeongrok
date: 2023-08-03 11:33:00 +0800
categories: [Paper, Review]
tags: [Paper]
pin: true
math: true
---


# 0. Abtract
---
- 기존의 Question Generation에 대한 지표 
  - BLEU, ROUGE, BERTScore, BLEURT 등
  - 이러한 지표는 참조 및 예측 질문 사이의 어휘적 중복 또는 의미 유사성이 많을 때 높은 점수를 부여.
- 기존 Question Generation 주요 단점
  1. 인간이 제공하는 비싼 `Gold Reference Question`이 필요.
  2. 참조 질문과 높은 어휘 또는 의미 유사성을 갖지 않을 수 있는 유효한 질문에 대해 패널티를 부과.
![img](https://www.researchgate.net/publication/365081715/figure/fig3/AS:11431281094645789@1667532013198/A-sample-of-re-ranking-experiment-that-the-annotator-prefers-the-best-candidate-chosen.png)

- 이 논문에서는, 후보 질문의 답변 가능성을 기반으로 새로운 지표인 RQUGE를 제안.
- 이 지표는 기존 연구에서 선 학습된 모델을 사용하는 질문 응답 및 범위 점수 모듈로 구성되어 있으며, `추가 훈련 없이 사용`할 수 있음. 
- RQUGE는 `Gold Reference Question`에 의존하지 않고 인간의 판단과 더 높은 상관 관계를 갖음을 보임.
- RQUGE는 여러 적대적 변조에 대해 더욱 견고하게 나타났으며, RQUGE로 재순위를 지정한 합성 데이터에 미세 조정함으로써 QA 모델의 성능을 상당히 향상시킬 수 있음을 보임

</br>
</br>

# 1. Introduction
---
- `Question Generation(QG)`의 목표는 답변 범위를 제공하거나 제공하지 않고 질문을 생성하는 것. 
- QG는 여러 응용 분야에서 사용될 수 있으며, 이러한 문제는 인간의 판단이 가장 정확하지만 비용이 많이 들고 확장하기 어려움. 
- 따라서 BLEU, ROUGE, BERTScore와 같은 여러 지표가 자동으로 생성된 텍스트의 질을 측정하기 위해 제안됨.

- 이 논문에서는 RQUGE라는 `Gold Reference Question`이 없는 질문 생성 평가 지표를 제안
- 이는 주어진 맥락과 답변 범위에 대해 Golde Reference Question이 필요하지 않는 후보 질문의 질을 계산 가능. 
- 여러 데이터셋을 통해 이 지표가 인간의 판단과 더 나은 상관 관계를 가짐을 보였으며, 이 지표는 적대적 전략에 대해 이전 지표보다 더 견고함.

요약하면, 이 논문의 기여는 다음과 같습니다:
• 참조 질문에 대한 액세스 없이 자동 생성 질문의 질을 측정하기 위한 RQUGE라는 평가 지표를 제안합니다.
• RQUGE가 인간 판단과의 상관 관계가 더 높음을 보입니다.
• RQUGE가 참조 질문의 부정, 엔터티 교환, 성별 변경 또는 의미 전달과 같은 적대적 전략에 대해 이전 작업보다 더 견고하다는 것을 보입니다.
• 합성 데이터에 RQUGE 지표를 적용함으로써 QA 모델의 성능을 크게 향상시킬 수 있다는 것을 보입니다.


</br>
</br>

# 2. Related Work
---
- 자연어 생성 (NLG) 작업의 자동 평가에 대한 이전 작업은 다음과 같이 분류:

1. Unspervised Metric
  - 이에는 가장 일반적으로 사용되는 메트릭인 BLEU, ROUGE, chrF, METEOR가 포함. 
  - 이러한 메트릭은 토큰 수준 일치 기능을 사용하여 참조 및 예측 시퀀스의 상관 관계를 이산 공간(discrete space)에서 계산. 
  - 최근 작업 예를 들어 BERTScore 및 MoverScore는 hard n-gram overlap 대신 BERT embedding을 사용하여 soft token-level 일치를 제공. - 이러한 메트릭은 다양한 NLG 작업에 적용.

2. Regression-based Metrics
  - 예로 COMET, BLEURT, S3, VRM은 supervised learning으로 회귀 계층을 훈련시켜 인간의 판단을 모방.

3. Ranking-based Metrics
  - 목표는 나쁜 예측에 비해 더 나은 후보에게 더 높은 점수를 부여하는 것. 
  - 가장 인기 있는 것들은 BEER

4. Generation-based Metrics
  - NLG 평가를 사전 훈련된 언어 모델에서 텍스트 생성 문제로 정의하는 아이디어.
  - 주어진 원본 시퀀스에서 더 나은 후보가 더 높은 점수(확률)로 생성되어야 함. 
  - 가장 인기 있는 것은 BARTScore와 PRISM
  - 또한, CTC와 QRelScore를 참조없는 메트릭으로 포함하여 더 나은 비교를 함.

</br>
</br>

# 3. RQUGE Architecture
---
![img](https://www.researchgate.net/publication/365081715/figure/fig1/AS:11431281094655294@1667532013136/The-architecture-of-RQUGE-metric-upper-side-for-the-question-generation-task-which.png)

- Question Answering, Span scorer Modules 두 부분으로 구성. 
- Context, Gold Answer Span, Candidate Question, Generated by a Question Generation Model(QG)를 기반으로 생성된 질문들에 대해서 `acceptance score`($$k$$)를 계산
  - $\begin{cases}
    a_c = QA(q_c, D)\\
    k = S(q_c, a_c, a_r, D)\\
  \end{cases}$

- 여기서 $$qc = QG(a_r, D)$$는 gold answer span $$a_r$$, 문맥 $$D$에 대한 생성된 후보 질문. 
- 점수를 계산하기 위해, 질문 응답 모델 $$QA(.)$$은 후보 질문 $$q_c$$와 문맥 $$D$$가 주어지면 답변 범위 $$a_c$$를 예측. 
- span scorer $$S(.)$$은 후보 질문, predicted answer, gold answer, context을 기반으로 acceptance socre $$κ$$를 계산


## 3.1 Question Answering Module
- 문맥과 후보 질문이 주어지면, `Question Answering Module`은 답변 범위를 예측. 
- 여러 도메인에 일반적으로 적용할 수 있도록, 우리는 UnifiedQAv2 모델을 사용하여 답변 범위를 생성. 
- UnifiedQAv2는 T5 기반의 인코더-디코더 모델로, 20개의 QA 데이터셋에서 훈련되었으며, 여러 내부 및 외부 도메인 데이터셋에서 최첨단 모델과 경쟁력 있는 성능을 달성합니다. 모델의 입력은 후보 질문과 해당 문맥의 연결입니다.
