---
title: A Context-based Chatbot Surpasses Trained Radiologists and Generic ChatGPT in Following the ACR Appropriateness Guidelines
author: jeongork
date: 2023-07-27 11:33:00 +0800
categories: [Paper, Review]
tags: [Paper]
pin: true
math: true
---


## Abstract
---
### Background
- 방사선 진단 이미지 가이드라인은 정확한 진단과 최적의 환자 관리에 필수적이다. 이 가이드라인은 표준화된 결정을 도출해 부적절한 이미징 연구를 줄인다. 
### Purpose
- 본 연구에서는 의미론적 유사성 처리를 사용해 American College of Radiology (ACR) 적합성 기준 문서로부터 개인화된 이미징 권장사항을 제공하는 대화형 챗봇을 이용해 임상 의사결정을 지원하는 잠재력을 조사했다. 
### Methods
- 우리는 209개의 ACR 적합성 기준 문서를 전문 지식 기반으로 사용하고, 대형 언어 모델을 외부 데이터와 연결하는 프레임워크인 LlamaIndex와 ChatGPT 3.5-Turbo를 사용해 적합성 기준 맥락 챗봇(accGPT)을 만들었다. 50개의 임상 케이스 파일을 사용해 accGPT의 성능을 다양한 경험 수준의 일반 방사선 전문가와 일반적인 ChatGPT 3.5 및 4.0과 비교했다. 
### Results
- 모든 챗봇은 적어도 인간 수준의 성능을 보였다. 50개의 케이스 파일에 대해, accGPT는 ACR 기준에 따라 '보통 적합한' 올바른 권장사항을 제공하는데 가장 탁월했으며, 또한 일반 챗봇과 방사선 전문가와 비교해 일관적으로 올바른 답변을 가장 많이 제공했다. 더욱이, 챗봇들은 상당한 시간과 비용 절약을 제공했으며, 모든 케이스의 평균 결정 시간은 5분, 비용은 0.19유로로, 방사선 전문가의 50분과 29.99유로에 비해 둘 다 p < 0.01이었다. 
### Conclusion
- ChatGPT 기반 알고리즘은 ACR 가이드라인에 따른 임상 이미징 연구에 대한 의사결정을 크게 개선할 잠재력이 있다. 특히, 맥락 기반 알고리즘은 일반적인 대체품에 비해 우수한 성능을 보였으며, 이는 특정 건강 관리 응용 프로그램에 AI 솔루션을 맞춤화하는 가치를 보여준다.
- ![img](https://pubs.rsna.org/cms/10.1148/radiol.230970/asset/images/medium/radiol.230970.va.gif)

## Introduction
---
- 현대 의료 분야에서는 정확하고 효율적인 진단 영상이 점점 더 필요하게 되어 의사 결정에 대한 표준화된 기준이 필요하다. 미국 방사선학회(ACR)는 1994년부터 이러한 권고안을 제공하고 있으며, 이를 통해 진료 의사의 의사 결정 과정이 간소화된다[1]. 이를 통해 환자 치료의 최적화, 진단의 정확성 향상, 방사선 노출 감소, 그리고 의료비용 감소가 가능해진다(https://www.acr.org/Clinical-Resources/ACR-Appropriateness-Criteria). 그러나, 이미징 자체의 필요성, 모달리티, 그리고 대조제 필요성에 대한 임상적 판단에서는 여전히 변동성이 존재하며, 이로 인해 부적절한 이미징 검사가 상당히 이루어지고 있습니다[2-5].

- 적절성 지침의 준수는 의료진과 방사선 전문가의 경험에 많이 의존하며, 인식 부족으로 인해 제한된다다(3,6–8). 또한, 이미징 기술의 빠른 발전과 임상 증거의 진보는 권장사항의 지속적인 업데이트를 필요로 하며, 이로 인해 그 사용이 더욱 복잡해진다.

- 여러 가지 임상 의사 결정 지원 도구가 소개되어 출판된 가이드라인의 사용을 향상시키는데 사용된다 (예: ESR iGUIDE (https://www.myesr.org/esriguide) 또는 CareSelect® Imaging; (https://www.changehealthcare.com/clinical-decision-support/careselect/imaging))[7,9]. 이런 도구들은 여러 임상 상황에서 환자의 진단 관리를 향상시키는데 유용하며, 부적절한 검사 비율을 줄이는데 효과가 있었다[2,10]. 그러나, 이들은 상당한 인간의 상호작용을 필요로 하며, 자유 텍스트 입력이 불가능하므로 관련 임상 정보를 잃을 가능성이 있다[2].

- 인공 지능 (AI) 기반 알고리즘은 대규모 언어 모델 (LLM)을 사용하여 이러한 한계를 해결할 수 있다. OpenAI는 2022년 11월에 ChatGPT를 대중에게 소개하였다. ChatGPT는 대화를 위해 특별히 훈련된 챗봇으로, 생성적 사전 훈련된 변환기와 최신 버전인 GPT 3.5-Turbo에 기반을 두고 있다. 특히 2023년 3월에 출시된 GPT 4는 상당한 의학적 지식을 제공하며 USMLE를 통과할 수 있음이 증명되었다[11,12].

- ChatGPT는 복잡한 정보를 빠르게 처리할 수 있으며, 이를 임상 방사선학 루틴에 적용할 수 있는 가능성이 이미 많이 탐색되고 발표되었다. 이에는 방사선학 보고서의 준비[13], 방사선학 보고서를 단순화하여 일반 언어로 변환[14,15], 차별 진단, 진단 절차, 최종 진단, 그리고 치료에 대한 임상 의사 결정 지원을 제공[16] 등이 포함된다.

- 이미 ChatGPT가 임상 의사 결정 지원으로서 이미징 권장사항을 제공할 수 있는 가능성에 대해 커뮤니티에서 토론하고 있으며, 유방암 스크리닝과 유방 통증에 대한 초기 데이터는 매우 흥미롭다[18]. 그러나 후자의 연구에서는 임상 파일이 아닌 ACR 가이드라인의 임상 상태에 대한 설명만이 제시되었으며, 인간의 성능과의 비교는 이루어지지 않았다. 또한, ChatGPT는 훈련 데이터(GPT 3.5-Turbo 및 GPT 4는 2021년 9월까지의 데이터에 대해 훈련됨)에 한정되어 있으므로, 가장 최신의 가장 전문적인 지식에 접근하지 못하거나, 훈련 데이터의 출처가 다양하므로 편향되어 있을 수 있다. 따라서 ChatGPT는 부정확하거나 불완전한 정보를 제공할 수 있다.

- 적절성 기준에 대한 특수한 지식을 포함하여 적절성 기준에 맞춘 GPT (accGPT)를 개발하는 것은 사용자의 질문에 대해 더 정확하고 관련성 있는 응답을 제공할 수 있을 것이다. 이 방법을 탐구하기 위해, 다양한 경험을 가진 일반 방사선학자들과 공개적으로 사용 가능한 일반적인 챗봇인 GPT 3.5-Turbo와 GPT 4를 ACR 적절성 기준의 지식으로 향상된 GPT 3.5-Turbo를 기반으로 한 accGPT 챗봇과 비교하여 벤치마킹하였다.

## Method
---
![img](https://pubs.rsna.org/cms/10.1148/radiol.230970/asset/images/medium/radiol.230970.fig1.gif)
### Data preparation and indexing
- ![img](https://pubs.rsna.org/cms/10.1148/radiol.230970/asset/images/medium/radiol.230970.fig2.gif)
- 제안된 accGPT를 개발하고 평가하기 위해, 우리는 진단 절차가 필요한 조건을 가진 209개의 주제를 포괄하는 ACR 적절성 기준을 기본 지식 베이스로 사용하였다(스키마는 그림 2에 제시되어 있습니다).
- 의미 유사성 처리를 위해, 우리는 외부 데이터인 ACR 적절성 가이드라인과 GPT 3.5-Turbo 사이의 인터페이스로 LlamaIndex를 사용하였다(https://github.com/jerryjliu/llama_index; 버전 0.5.0). 텍스트 정보는 ACR 가이드라인에서 추출되었고, 임베딩은 LlamaIndex의 GPTSimpleVectorIndex를 사용하여 생성되었다. 이를 위해, 문서 텍스트는 최대 512 토큰(텍스트 내용의 측정치)까지의 작은 덩어리로 나뉘고 텍스트 노드로 변환되었다. 이러한 노드는 임베딩이라는 과정을 통해 숫자 벡터로 변환되었고, 결과는 딕셔너리와 유사한 구조인 벡터 인덱스에 저장되었다.

### Prompting strategy and answer synthesis
- 모든 챗봇의 출력을 사례 기반 시나리오에 맞게 조정하기 위해, 각 경우에 시스템에 제기된 질문은 다음과 같았다: "이 경우에 이미징이 일반적으로 적합한가요? 그렇다면, 가장 적절한 이미징 방식을 명시하고, 대조제가 필요한지 여부를 알려주세요. '적절할 수 있음'과 '일반적으로 적절하지 않음'을 잠재적인 응답으로 제외해 주세요."
- GPT 3.5-Turbo와 GPT 4에 대해서는 직접적인 출력이 응답으로 캡처되었다. 우리의 문맥 기반 챗봇에 대해선, 프롬프트의 임베딩을 기반으로 인덱스에서 가장 잘 매치되는 데이터 노드 세 개가 검색되었다. 이 노드들은 GPT 3.5-Turbo를 사용한 다단계 답변 생성 및 정제 방법에 사용되었고, 최종 출력은 그 후 캡처되었다.

### Preparation of case files
- 우리는 임상 일상과 유사한 시나리오에서 챗봇의 정확성을 사람의 성능과 비교해 보고자 했다(워크플로우는 그림 2에 나와 있습니다). 이를 위해 ACR 적절성 기준에 따라 50개의 임상 사례 파일을 만들었다. 잠재적으로 진단 절차를 필요로 하는 209개의 주제 중 50개를 무작위로 선택했다. 그런 다음 각 주제에서 하나의 변형을 무작위로 선택했다. 이 50개의 임상 상황을 바탕으로, 임상 일상적인 소견서와 유사한 사례 파일을 만들었는데, 이 작업은 읽는 것에 참여하지 않은 경험 많은 방사선과 의사가 수행했다. 임상 파일에는 대부분의 경우 의심되는 병리학을 제외한 나이, 성별, 주요 불만, 과거 의료 이력, 임상 검사 결과 등의 정보가 포함되었다. 사례 파일은 다양한 주제와 의료를 다루고 있었으며, 이 중 일부는 방사선과 의사의 일상적인 진료에서는 드물게 마주치는 것들이었다(자세한 내용은 부록 S1을 참조하십시오).

### Assessment of human and chatbot performance
- 50개의 사례 파일은 ACR 가이드라인에 익숙한 여섯 명의 일반 방사선과 의사들에게 제시되었는데, 이들은 경력 수준이 다르며, 훈련 첫 해와 둘째 해의 초창기 방사선과 의사 두 명, 훈련 마지막 해의 고급 레지던트 두 명, 그리고 경력 11년과 12년의 보드 인증 방사선과 의사 두 명이었다. 각 사례 파일에 대해, 모든 방사선과 의사들은 독립적으로 영상 적절성, 필요한 경우 가장 적절한 영상 모달리티, 그리고 대비제 투여 필요성을 평가했다. 이 평가 과정에서는 동료나 가이드라인과의 상의가 허용되지 않았다.
- 우리는 50개의 사례 파일에 대해 스크립트 기반 접근법을 사용하여 모든 세 챗봇에 대해 6회 반복 테스팅을 수행하고 그들의 성능을 평가했다. 다시 한번, 영상의 적절성, 필요한 경우 가장 적절한 영상 모달리티, 그리고 대비제 투여 필요성을 평가했다.

### Accuracy and agreement of rediologists and chatbots
- 50개의 사례 파일에 대한 각각의 인간과 챗봇에 의한 영상 추천은 그들의 적절성에 대해 ACR 가이드라인에 따라 평가되었다, 즉 그들이 "일반적으로 적절함" 또는 "적절할 수 있음" 기준을 충족하는지 여부. 인간 평가자, GPT 3.5-Turbo, GPT4, 그리고 accGPT를 비교하기 위해, 우리는 "올바른 등급 여부"라는 이진 결과를 위해 일반화된 선형 혼합 모델 (이항 가족과 로짓 링크)을 적합시켰다. 결과에 관해서는 "적절할 수 있음"이 "예" 또는 "아니오" 카테고리에 포함되는지 여부에 대한 별도의 모델을 적합시켰다. 등급 방법 (인간, GPT 3.5-Turbo, GPT4, accGPT)은 고정 요인으로 포함되었고, 사례와 평가자에 대한 무작위 절편을 허용했다. 우리는 모든 쌍 비교를 얻기 위해 방법의 참조 그룹을 교대로 사용했다. 추정은 GNU R (버전 4.2.1)의 lm4 패키지 (버전 1.1-33)의 glmer() 함수를 사용하여 수행되었다. 쌍 비교에 대한 p 값은 `asymptotic Wald` 테스트를 사용하여 계산되었습다.
- GPT 알고리즘을 통한 평가의 일관성에 대한 인상을 얻기 위해, 우리는 각 사례를 6번 제시하고 100% (6 중 6) 올바른 등급, 그리고 적어도 66.66% (6 중 4) 올바른 등급을 받은 사례의 비율을 계산했다.

### Assessment of cost-effectiveness of radiologists and chatbots
- 또한, 비용 효과성을 평가하기 위해 읽는 사람별로 결정까지의 시간을 평가했다. 방사선과 의사의 비용 계산은 독일 대학 병원에서 일하는 의료 의사들의 공개적으로 이용 가능한 급여 정보에 기반하여 이루어졌다 (https://oeffentlicher-dienst.info/aerzte/uniklinik/). GPT 모델을 사용하는데 발생하는 비용은 OpenAI 웹페이지의 청구 출력 (https://platform.openai.com/account/usage)에서 모니터링되었으며, 첫 번째 실행 중의 토큰 사용량 및 가격 목록으로 검증되었다.

### Code availability
- 우리의 챗봇 구현에 대한 소스 코드는 오픈소스 MIT 라이선스 (https://github.com/maxrusse/accGPT)하에 GitHub에서 공개적으로 이용 가능하다. 연구 및 기타 프로젝트에서의 코드 사용은 라이선스의 조건에 따라 이루어져야 한다.


## Results
---
### Human and Chatbot Performance
- 요약하자면, 챗봇들은 표 1과 그림 3에 주어진 인간의 성능 수준에서 최소한으로 수행되었다. 케이스별 답변 세부사항은 표 1과 표 S1에서 제공된다.
- ![img](https://pubs.rsna.org/cms/10.1148/radiol.230970/asset/images/medium/radiol.230970.tbl1.png)
- ![img](https://pubs.rsna.org/cms/10.1148/radiol.230970/asset/images/medium/radiol.230970.fig3.gif)
- “일반적으로 적합한” 기준을 만족하는 50개의 사례 파일에 대한 영상 추천을 제공하는 accGPT의 성능은 방사선과학자들과 GPT 3.5 Turbo보다 훨씬 좋았다 (각각 odds ratio (OR) 3.76 및 2.93; 둘 다 p<.001). GPT 4에 비해 accGPT는 경향적으로만 더 좋은 성능을 보였습니다 (OR 1.54; p=0.08). 더 많은 세부사항은 표 2에서 제공된다.
- ![img](https://pubs.rsna.org/cms/10.1148/radiol.230970/asset/images/medium/radiol.230970.tbl2.png)
- "일반적으로 적합하다"와 "적합할 수 있다"라는 기준을 만족하는 추천들을 비교했을 때, accGPT는 다시 한번 방사선과학자들과 GPT 3.5 Turbo보다 우수한 성능을 보였다 (각각 OR 1.35, p=0.001 및 OR 2.02, p=0.00599), 반면 GPT 4의 성능은 차이가 없었다 (OR 0.78, p=0.39). 자세한 내용은 표 3을 참조하십시오.
- ![img](https://pubs.rsna.org/cms/10.1148/radiol.230970/asset/images/medium/radiol.230970.tbl3.png)
- 방사선과학자를 다른 경험 수준으로 분류했을 때, 우리는 정확한 답변에서 견고한 차이를 관찰하지 못했으며, 랜덤 인터셉트의 분산이 거의 제로였고, 인터셉트를 제외하면 고정 효과에 대한 결과에 변화가 없었음을 의미하여 경험 수준이 영향을 미치지 않았다.

### Consistency of radiologists and chatbots
- 표 1에 주어진 것처럼, 일반적인 GPT 3.5-Turbo와 GPT 4는 6번의 반복 실행에서 "적합할 수 있다"는 추천의 정확한 수를 accGPT보다 많이 제공했다. "적합할 수 있다"를 잘못된 추천으로 간주할 경우, accGPT는 모든 경우의 74%에서 6번의 실행을 모두 올바르게 하였으며 (95% CI: 62%-86%), 모든 경우의 82%에서 최소 4번은 올바른 추천을 제공하였다 (95% CI: 71%-93%). GPT 3.5-Turbo는 각각 42% (95% CI: 28%-56%)와 68% (95% CI: 55%-81%)의 올바른 추천을 제공하였으며, GPT 4는 각각 64% (95% CI: 51%-77%)와 78% (95% CI: 67%-90%)의 올바른 추천을 제공하였다. “적합할 수 있다”를 정확한 것으로 간주할 경우, accGPT는 모든 경우의 74%에서 6번의 실행을 모두 올바르게 하였으며 (95% CI: 62%-86%), 모든 경우의 84%에서 최소 4번은 올바른 추천을 제공하였다 (95% CI: 74%-94%). GPT 3.5-Turbo는 각각 54% (95% CI: 40%-68%)와 76% (95% CI: 64%-88%)의 올바른 추천을 제공하였으며, GPT 4는 각각 76% (95% CI: 64%-88%)와 86% (95% CI: 76%-96%)의 올바른 추천을 제공하였다.

### Analysis on cost-effetiveness of radiologists and chatbots
- 방사선과학자들은 사례 파일을 평가하는데 다양한 시간을 소비했으며, 평균 지속 시간은 49:48분 (SD 19분; 범위 22-73분)이었다. 이로 인해 평균 비용은 29.99€ (SD 12.78€, 범위 13.2-54.86€)로 산정되었다. GPT 3.5-Turbo는 2분이 소요되며 비용은 0.02€, GPT 4는 5분이 소요되며 비용은 0.36€, accGPT는 8분이 소요되며 토큰 비용은 0.18€였다. 전반적으로, 시간과 비용은 방사선과학자들에 비해 챗봇들에서 훨씬 낮았다 (둘 다 p=0.003).


## Discussion
---
- 우리의 연구 결과는 표준적인 임상 의뢰 메모를 받아들이고 간결한 이미징 권고안을 제공하는 엔드 투 엔드 솔루션으로서 ACR 지침에 기반한 이미징 권고안을 제시하는 컨텍스트 기반 accGPT 챗봇의 잠재력을 보여준다. 다양한 경험을 가진 일반 방사선학자들과 두 가지 공개적으로 이용 가능한 일반적인 챗봇 (GPT 3.5-Turbo 및 4.0)과 비교했을 때, 보다 높은 정확도와 더 일관된 "일반적으로 적합한" ACR 권고안을 충족시키는 능력을 확인했다. 또한, 모든 챗봇들은 사례 파일 평가에 있어서 인간 방사선학자보다 훨씬 시간 효율적이고 비용 효율적이었다.

- 우리의 연구 주제는 지침의 부족한 사용으로 인해 부적합한 이미징이 대량으로 이루어지는 문제 때문에 매우 중요하다[2,5]. 부적절한 이미징은 의료 시스템에 대한 비용 증가, 대기 리스트의 연장, 잘못된 또는 지연된 진단, 그리고 잠재적으로 불필요한 이온화 방사선 노출과 관련이 있다[6].

- 특히, 모든 평가된 챗봇들은 최소한의 인간 수준의 성능을 보였으며, accGPT는 일관되게 가장 높은 비율의 "일반적으로 적합한" 권고를 제공했다. accGPT가 인간 평가자들과 다른 챗봇들보다 더 적게 "적합할 수 있는" 권고안을 제공했음을 강조해야 하는데, 이는 권고안의 질을 반영한다. 방사선학 보고서의 간소화(15)와 USMLE 문제 해결(12)에 초점을 맞춘 다른 연구에서도 GPT 버전 3.5에서 4.0으로의 성능 개선이 확인되었다. 따라서 GPT 4를 활용한 컨텍스트 기반 챗봇의 성능 향상을 상당히 기대할 수 있다. 또한, 상업용 벡터 저장소와 최신 OpenAI 모델의 전용 조합에 대한 초기 개발 단계가 제한된 알파 단계에서 진행 중이다 (https://openai.com/blog/chatgpt-plugins).

- 그러나 ChatGPT 자체는 특별히 임상 결정을 위해 설계된 것은 아니다. 이로 인해 잘못된 권고가 제시되거나 "어휘 외" 문제를 야기하여, 완전히 정확하거나 현실과 일치하는 결과를 내놓지 못할 수 있다. 그러나 ACR 지침과 같은 전문적인 컨텍스트 기반 지식을 통합함으로써 이 제약을 해결할 수 있다. 이 적응성은 챗봇의 지식 베이스가 다른 신뢰할 수 있는 출처로부터의 추가 지침이나 권고안으로 업데이트되거나 보완될 수 있게 한다. 이는 가짜 뉴스와 잘못된 데이터와 관련된 우려를 해소하고, 챗봇의 출력의 신뢰성을 보장한다[17,19].

- 우리는 방사선학자들과 챗봇들 모두에서 특히 이미징이 필요하지 않은 경우(예: 사례 파일 6, 23, 35)에 도전이 있음을 확인했다. Rao 등은 이러한 한계점을 지적하였으며, 그들의 연구에서는 ChatGTP가 필요하지 않은 경우에도 이미징을 권장했다[18].

- ACR 적합성 기준은 주로 어떤 형태의 이미징이 필요한 임상 상태들을 포괄하기 때문에, 이미징이 필요하지 않은 더 많은 사례 신상들(예: 피부 병리학적 상태)을 포함하는 추가 조사가 필요하다.

- ESRiGuide와 같이 오직 신뢰할 수 있는 출처만을 기반으로 하는 기존 도구들은 사례 신상 정보를 입력을 위해 간소화해야 하는 단점이 있다. 이는 이전 검사와 진단 절차에 대한 정보를 평가하는 것을 방해한[9]. 반면에, 조사된 챗봇들은 의사들의 의뢰 텍스트를 직접 입력할 수 있게 한다. 또한, ChatGPT 기반 접근법은 최근에 보여진 것처럼 의사결정 과정에 대한 더 깊은 이해를 가능하게 한다[12]. 이는 이미 개발된 임상 의사 결정 지원 시스템으로서의 덜 발달된 자연어 처리 접근법보다 우월하다[20].

- 제안된 접근법의 또 다른 이점은 방사선학자와 비교하여 평가 기간의 감소와 알고리즘의 두드러진 비용 효율성에 관한 것이다. 기술적인 관점에서, 임상 정보 시스템이나 방사선학적 예약 시스템에 통합하는 것이 가능하다. 방사선학자의 업무 부담이 계속 증가함을 고려할 때, accGPT와 같은 통합 도구들이 임상에서의 안식처가 될 수 있다[21].

- accGPT와 같은 AI 기반 솔루션들을 의료 분야에 도입할 때는 윤리적, 법적 그리고 데이터 보안 고려 사항들을 다루는 것이 중요하다. 또한, 임상 의사결정에서 AI의 사용은 투명성, 책임성, 그리고 잠재적인 편향에 대한 우려를 일으킨다[17,22]. 컨텍스트 기반 챗봇의 경우, 참고 자료를 추가로 표시하는 의사결정 지원 도구로서의 사용이 투명성을 증가시키고 신뢰를 강화할 수 있다.

- 제안된 accGPT의 추가 최적화는 비용, 이용 가능성, 그리고 잠재적인 방사선 용량에 대한 상세한 평가를 포함해야 한다. 우리는 accGPT가 방사선학자와 의뢰 의사 모두에게 유익할 수 있을 것으로 보이지만, 기대되는 주요 사용 사례는 다르다. 방사선학자들은 주로 희귀한 사례에 대한 정보 검색 도구로 사용하게 될 것이며, 의뢰 의사들은 의뢰 과정에서의 결정을 안내하는 빠른 참조로 이용할 수 있을 것이다. 이는 ACR 지침의 수동 검색을 현재 사용하는 사람들과 일치한다. 성능이 현재로서는 좋지만 훌륭하지 않기 때문에, 의뢰 메모의 완전히 자동화된 검토는 아직 실행 가능하지 않다. 그러나 이들이 accGPT에 의해 검토되고 의뢰 의사와 방사선학자에게 지원적인 권고가 제시될 것으로 상상할 수 있다.

- 인간 평가자들의 제한된 전반적 성능은 대체로 ACR 지침이 평가 동안 참고로 이용 가능하지 않았기 때문인 것으로 보인다. 또한, 사례 파일은 일반적인 방사선학자들에 의해 평가되었으며, 많은 사례 파일들은 희귀한 집합에 관련되었다. 이러한 경우, 하위 전문화된 방사선학자들에게서 더 나은 성능을 기대할 수 있다. 그러나 일반 방사선학자들을 포함시킴으로써 ACR 지침의 전체적인 범위를 커버할 수 있었다. 이 광범위함이 경험있는 방사선학자와 주니어 방사선학자 사이의 우위를 설명하지 못하는 이유일 수도 있다. 이 희귀한 사례들은 ACR 지침의 상담을 필요로 했을 것이지만, 이것은 이 연구에서 가능하지 않았다. 평가 동안 ACR 지침에 접근할 수 있었던 인간 평가자들의 성능 분석이 부족하다. 여기서 높은 성능을 가정할 수 있지만, 시간의 상당히 높은 소요도 예상된다. 추가로, 데이터셋이 상대적으로 작다는 사실과 우리가 합성 사례 파일들을 사용했다는 점이 한계가 될 수 있다. 그러나 이를 통해 ACR 지침에서 임상 상태를 무작위로 선택할 수 있었고, 이는 희귀한 임상 상태를 포함하여 ACR 지침의 전체 범위를 커버할 수 있게 하였다.

- 요약하면, ChatGPT 기반 알고리즘들은 ACR 지침에 따른 임상 이미징에 대한 의사결정을 상당히 개선할 수 있는 잠재력이 있다. 특히, 컨텍스트 기반 알고리즘은 일반적인 챗봇보다 우월하였으며, 이는 특정 의료용 애플리케이션에 AI 솔루션을 맞춤화하는 가치와 특정 작업을 수행하기 위해 일반적인 챗봇을 강화하는 잠재력을 보여준다


## Appendix
---
### Workflow
1. Case Selection
  - 미국 방사선학회(ACR)의 적절성 기준 가이드라인에서 무작위로 50개의 다른 임상 상태를 선택했다. 여기서, 이 연구의 테스트 데이터셋을 제공하기 위해 모든 관련 정보를 포함하는 사례 파일들이 생성되었다. 이 표본 추출 전략은 다양한 주제 범위를 포괄하도록 설계되었다.
  - 예를 들어, 34번 사례 파일은 ACR 적절성 기준 주제인 “Nonatherosclerotic Peripheral Arterial Disease(비동맥경화성 말초동맥질환)” 변형 1 “Suspected popliteal entrapment syndrome(의심되는 무릎동맥 증후군. 초기 영상.)”을 기반으로 한다. 여기에서 합성 사례 파일이 생성되었다:
    - 환자 나이: 24세
    - 성별: 여성
    - 주요 불만: 활동 중 하지에서의 불편함. 
    - 환자는 몇 달 동안 진행되어 온 신체 활동 중 왼쪽 하지의 통증에 대해 불만을 제기한다. 환자는 열성적인 러너로, 지난 1년 동안 마라톤 대회를 준비하고 있다. 그녀는 주요한 의료 이력이 없으며 약물을 복용하지 않다. 신체 검사에서 환자의 생체징후는 정상이다. 하지 부종은 없다. 왼쪽 하지가 오른쪽보다 약간 작아 보이며, 무릎관절하부에 약간의 압통이 있다.

  - ACR 적절성 기준에 따르면, “보통 적절한” 방법은 IV 조영제 없이 및 있이 하지의 MRA, 하지의 초음파 이중 도플러 및 IV 조영제가 있는 하지의 CTA이며, IV 조영제 없이 하지의 MRA 및 하지의 동맥조영술은 “적절할 수 있다”이다.

2. Assesment of Human Radiologists
  - 6명의 다양한 경험을 가진 일반 방사선 전문가들이 통제된 환경에서 50개의 사례 파일을 평가하였다. 각 방사선 전문가는 브라우저 기반 설문조사를 완료하여, 각 사례에 대한 진단적 추천을 제공했다. 이 평가는 추가적인 자료 활용 및 동료들과의 상담이 금지되었으며, 설문지를 완료하는 데 걸린 시간이 정확히 기록되었다.
  - 예를 들어, 한 명을 제외한 모든 인간 방사선 전문가들은 조영제를 사용하지 않는 방사선 촬영을 올바르게 권장했다.

3. Assessment of Generic Chatbots
  - OpenAI의 최신 언어 모델인 GPT 3.5-Turbo와 GPT-4가 모델 개선이나 적응 없이 50개의 사례를 처리하는 데 사용되었다. 모델들은 각 사례에 대한 영상 적절성을 평가하였고, 필요한 경우, 가장 적합한 영상 방식과 조영제 필요성을 제안하였다. 이를 위해, 모델들은 인간 설문지와 유사하게 다음과 같은 프롬프트를 받았다. 각 모델은 응답의 신뢰성을 평가하기 위해 6번의 테스트 주기를 거쳤다.

4. Assessment of accGPT
  - 전문 지식으로 향상된 챗봇의 잠재력을 평가하기 위해, 우리는 적절성 기준을 적용한 GPT (accGPT)를 개발하였다. 이 챗봇은 GPT 3.5-Turbo와 정보와 임베딩의 ACR 적절성 기준을 기반으로 하는 벡터 인덱스를 결합한 것이다. 이러한 인덱스는 대규모 텍스트 인덱싱 및 검색을 위해 설계된 파이썬 라이브러리인 라마 인덱스를 사용한다. 작업흐름은 그림 2에서 보여지며, accGPT 모델의 방법론에서 자세한 설명이 제공된다. 각 사례에서는 입력 쿼리(프롬프트와 각각의 사례 파일)를 기반으로 임베딩이 생성되었다. 이 사례 특정 임베딩, 즉 숫자의 벡터는 벡터 인덱스에 있는 임베딩과 비교되어 가장 가까운 세 가지 일치 항목을 정의한다. 이 최상위 세 가지 일치 항목에서, 텍스트 노드들이 추출되어 답변 생성을 위해 전달된다. 프롬프트, 사례 설명, 그리고 추출된 세 개의 텍스트 노드의 조합이 답변을 생성하기 위해 언어 모델 GPT 3.5-Turbo에 새로운 입력으로 사용된다. 이 챗봇 역시 6회의 테스트 주기를 거쳤다.

5. Evaluation of Performance
  - 일반 방사선 전문가와 챗봇들이 제공한 진단적 추천들은 ACR 적절성 기준과 비교되었다. 평가는 추천의 적절성을 고려했으며, 만약 제안된 영상 방식 또는 조영제의 필요성이 ACR 가이드라인과 다르면, 그것은 잘못된 것으로 분류되었다.

### Mehthodology of the accGPT Model(accGPT 모델의 방법론)
- accGPT 모델은 OpenAI의 GPT 3.5-Turbo 모델의 강점을 활용하고 ACR 가이드라인의 전문 지식과 벡터 인덱스를 결합하여 특정 방사선학적 사례에 대한 관련성 있는 응답을 제공하는 혁신적인 접근법이다. 다음에 우리는 accGPT를 실행하는 단계별 접근법을 제공한다.
- Importing the ACR guidelines(ACR 가이드라인 가져오기)
  - 처음에는 ACR 적절성 기준 가이드라인이 PDF 파일로 다운로드되었다. 이 PDF 파일들은 그 다음 텍스트 노드라고 부르는 더 작은 텍스트 세그먼트로 변환되었으며, 각 텍스트 노드는 최대 512개의 토큰을 포함한다. 토큰들은 언어 모델들이 텍스트를 이해하는데 사용되는 단어의 조각이며, 항상 완전한 단어에 해당하지는 않는다. 우리의 경우, 512개의 토큰은 대략 384개의 단어에 해당했다.

- Embedding
  - 그 다음, 텍스트 노드들은 임베딩이라는 과정을 통해 수치적 벡터로 변환되었다. 이 과정은 OpenAI에서 제공하는 텍스트-임베딩-ada-002 모델을 사용하여 실행되었다. 이 과정은 모델이 다차원 공간에서 의미적 유사성과 뉘앙스를 포착하고 표현할 수 있게 했다. 좀 더 자세한 설명을 위해서는 https://openai.com/blog/new-and-improved-embedding-model 을 참조.

- Vector Index
  - 임베딩 이후, 텍스트는 파이썬 라이브러리 LlamaIndex 버전 0.5.0(https://github.com/jerryjliu/llama_index)를 사용하여 벡터 인덱스에 저장되었다. 이 인덱스는 ACR 가이드라인의 각 텍스트 노드와 그에 해당하는 임베딩을 짝지어 높은 차원의 의미적 맵을 만들었다. 벡터 인덱스는 ACR 가이드라인을 그들의 벡터 표현과 연결하며, 특정 사례가 제시될 때 의미적 유사성에 기반한 관련 정보의 빠르고 정확한 검색을 가능하게 한다.

- Forming Response
  - 각 사례에 대해 동일한 프롬프트가 입력에 추가되어 작업을 정의했다: 
    - Is imaging typically appropriate for this case? If so, pelase specify the most suitable imaging modality and whether a contrast agent is required. Exclude 'May Be Appropriate' and 'Usually Not Appropriate' as potential responses.
  - 그리고 각 주어진 사례에 대해, 쿼리의 본질을 대표하는 고유한 임베딩이 생성되었다. 이는 특정 상세 사항을 포착했다. 이 계산된 임베딩은 그 다음 벡터 인덱스에 저장된 모든 것과 비교되어 의미적 유사성을 기반으로 세 가지 가장 가까운 일치 항목을 식별하였다. 이 과정은 입력 사례의 맥락에 가장 일치하는 가이드라인의 텍스트 노드를 선택할 수 있도록 했으며, 이는 언어나 어구의 변형에 관계없이 이루어졌다. 이렇게 식별된 상위 세 가지 일치 항목의 관련 텍스트 노드는 그 다음 추출되어 응답 생성 과정을 위해 준비되었다. 이 응답의 생성은 초기 프롬프트, 사례 설명, 그리고 회수된 텍스트 노드를 결합하였다. 이런 식으로 모은 데이터는 그 다음 GPT 3.5-Turbo 언어 모델에 입력되어, 최종적인 포괄적인 응답을 생성하도록 사용되었다.

- Publication and Useer Interface
  - 접근성과 투명성을 촉진하기 위해, accGPT 모델의 소스 코드가 사용자 인터페이스 버전의 일부로 공개적으로 GitHub(https://github.com/maxrusse/accGPT)에 공개되었다. 이 인터페이스는 파이썬 라이브러리 Gradio를 사용하여 만들어졌으며, 응답을 생성하는 편리한 방법을 제공하고 GPT 3.5-Turbo, GPT-4, 그리고 accGPT 모델 간의 비교를 가능하게 한다. 공유된 코드는 다른 가이드라인을 기반으로 하는 챗봇을 생성하는데 대한 지표로써 제공될 수 있으며, 인터페이스와 애플리케이션 프로그래밍 인터페이스(API)의 사용을 예시로 들어 더 정교한 소프트웨어 솔루션에 통합하는 방법을 보여준다.
