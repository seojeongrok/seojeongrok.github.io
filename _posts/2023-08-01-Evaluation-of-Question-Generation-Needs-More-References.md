---
title: Evaluation of Question Generation Needs More References
author: jeongrok
date: 2023-07-27 11:33:00 +0800
categories: [Paper, Review]
tags: [Paper]
pin: true
math: true
---


## Abstract
---
- Question Generation(QG)은 주어진 문맥과 목표 답변에 기반하여 유효하고 유창한 질문을 생성하는 작업. 
- 다양한 목적에 따라, 동일한 문맥이 주어져도 교사들은 다른 개념에 대한 질문을 할 수 있으며, 동일한 개념도 다양한 방식으로 표현될 수 있음. 
- 그러나, QG의 평가는 보통 n-gram 기반 메트릭이나 학습된 메트릭 같은 단일 참조 기반의 유사성 메트릭에 의존하는데, 이는 QG 방법의 잠재력을 완전히 평가하기에는 충분하지 않음. 
- 이에 대한 해결책으로, 우리는 참조 질문을 의미적이고 구문적으로 다양한 방식으로 재구성하여 더욱 견고한 QG 평가를 제안. 
- GPT-3과 같은 대형 언어 모델을 사용하여 의미와 구문적으로 다양한 질문을 만들고, 인기 있는 평가 메트릭의 간단한 집계를 최종 점수로 채택. 
- 실험을 통해, 우리는 `multiple (pseudo) references`를 사용하는 것이 QG 평가에 더 효과적이며, `single reference`로 평가하는 것보다 인간의 평가와 더 높은 상관관계를 보여주는 것을 발견.


## 1. Introduction
---
- 질문 생성(QG)은 주어진 텍스트와 관련이 있고 그 텍스트에 의해 답변할 수 있는 질문을 생성하는 작업. 
- QG는 교육 시나리오뿐만 아니라 질문-답변 작업을 개선하는데도 적용될 수 있으므로, QG 프레임워크와 그들의 자동 평가를 설계하는 것에 대한 관심이 증가.

- 그러나 이전의 QG 작업들은 대부분 생성된 질문이 골드 참조 질문과 얼마나 비슷한지에 따라 자신들의 방법을 평가하며, n-gram 기반의 유사성 메트릭을 사용. 
- `single reference`만 주어진다면 이러한 메트릭들은 질문의 어휘적이고 의미적인 다양성을 고려하지 않으며, 인간의 판단과 낮은 상관관계를 보인다. 

- 이러한 다양한 `gold reference question`을 명확하게 비교하기 위해, 우리는 QG 프레임워크를 평가하기 위한 단일 참조 질문을 보강하는 것을 제안하며, 이를 다중 참조 평가(MRE)라고 부릅니다. 이는 GPT-3와 ChatGPT와 같은 대형 언어 모델의 `few shot` 능력을 활용합니다. 

  -  `gold reference question`
      - 특정 문맥이 주어지고 질문을 생성했을 때, 생성된 질문의 품질을 평가하기 위한 기준이 되는 질문.
      - 일반적으로 사람에 의해 작성되거나 검토를 받아서 품질이 높다고 인정받는 질문
      - QG는 일반적으로 `gold reference question`과 얼마나 유사한지를 측정하여 모델의 성능을 평가


- 우리는 이를 `reference augment`을 활용하여 QG 프레임워크를 평가하는 최초의 적용으로 알고 있습니다. 우리의 주요 기여를 다음과 같이 요약합니다:
  - 우리는 질문의 구문적이고 의미적인 변형을 명확하게 고려할 수 있는 다중 참조 평가(MRE)를 위해 단일 참조를 보강하는 것을 제안. 퀴즈 설계 데이터셋에서의 실험 결과, MRE가 적용될 때 기존 메트릭의 성능이 상당히 향상될 수 있음을 보여줌.
  - MRE는 메트릭에 대해 중립적이므로, 우리의 방법을 통해 다양한 메트릭이 개선될 수 있다. 각각의 기존 메트릭이 다른 통찰을 제공할 수 있으므로, MRE는 이러한 다양한 관점을 개선하는 데 사용될 수 있다.
  - 우리는 보강된 참조 질문을 부록으로 제공하여, 추가 연구를 위해 우리의 결과를 재현하는 기회를 제공. 우리는 또한 인간의 주석가에 의해 보강된 참조들이 올바른지 확인하였습니다.

## 2. Method
---
### 2.1 Single Reference Evaluation
- QG 평가의 이전 연구에서는 생성된 질문 $$q^g$$의 품질을 골드 참조 질문 $$q^r$$에 대해 $$M(q^g,q^r)$$ 측정한다. 
- 여기서 M은 BLEU 및 ROUGE-L과 같은 QG 평가에서 널리 사용되는 유사성 메트릭을 나타낸다. 
- 그러나 이러한 메트릭들은 오직 하나의 `gold reference question`만을 가정하므로, 적절한 질문도 낮은 점수(`false positive`)가 부여될 수 있습니다.


2.2 다중 참조 평가 (MRE)
- 이 문제를 해결하기 위해, 우리는 다중 참조 평가를 제안 
- 후보 질문 $$q^g$$가 다수의 참조 $$Q = \lbrace q_0^r,q_1^r,...,q_N^r \rbrace$$:
  - $$s = max M(q_i^r,q^g) for i = 0,...,N$$

- 다양한 `gold reference question`과 기존 metric을 비교함으로써 QG 프레임워크의 더 실질적인 능력을 측정할 수 있다. 이 방법은 어떠한 유사성 기반 메트릭에도 적용할 수 있기 때문에, 생성된 질문의 다양한 특성을 보여주는 메트릭으로부터 유익한 인사이트를 얻을 수 있다.



그러나, 이러한 다중 참조를 인간 어노테이터로 수집하는 것은 실용적이지 않으므로, 우리는 최근의 대형 언어 모델, 특히 GPT-3와 ChatGPT를 활용하여 Q를 Qˆ로 대체합니다. 참조 질문 q0r가 주어지면, 우리는 그것을 N개의 질문으로 확장합니다:
Qˆ = LLM(q0r). (2) 1ACL2023의 발견에서 발행 예정
우리는 (Liu et al., 2021)에서와 같이 컨텍스트와 질문의 쌍이 아니라 골드 질문 q0r만을 제공합니다. 이는 LLM의 제로샷 QG 능력이 교육적 목적에 위험하다고 보고되기 때문입니다 (Wang et al., 2022b). 따라서 우리는 LLM을 다시말하기 생성기로 사용하며, 이는 LLM에 대한 다시말하기와 훈련 패러다임 사이에 높은 상관관계가 있다고 보고되었습니다 (Chen et al., 2022).

GPT-3가 제로샷 설정에서 ChatGPT보다 떨어지므로, 여기서 우리는 GPT-3의 컨텍스트 내 학습 능력을 활용하며, 이를 위해 우리는 GPT-3에게 Appendix A와 같은 세 개의 ChatGPT로 다시말한 질문들을 시연합니다. 우리는 또한 실험(섹션 3.6)에서 다시말한 질문의 정확성을 더욱 조사할 것입니다.

3 실험
3.1 데이터셋과 평가
MRE의 효과성을 확인하기 위해, 우리는 자동 질문 평가와 인간 어노테이션 사이의 상관 관계를 측정하기 위해 퀴즈 설계 데이터셋 (Laban et al., 2022)를 사용합니다. 퀴즈 설계 데이터셋은 컨텍스트, 답변, 그리고 자동으로 생성된 질문들로 구성된 3,164개의 인간 어노테이션 샘플을 포함합니다. 각 샘플에 대해, 인간은 질문이 유창한지, 주어진 답변을 도출할 수 있는지, 그리고 맥락에 맞는지(1) 혹은 아닌지(0)를 어노테이션합니다.

우리는 질문의 골드 인간 점수를 [0, 1]의 이산 인간 어노테이션의 평균으로 정의합니다. 그런 다음, 우리는 주어진 구절에 대한 참조 질문으로 인간 점수가 1인 질문을 선택합니다. 마지막으로, 나머지 질문에 대해, 우리는 인간 점수와 자동 평가 점수 사이의 피어슨 상관 계수 (Freedman et al., 2007)와 스피어만 순위 상관 계수 (Zar, 2005)를 측정합니다.